---
title: "Modeling to Predict Wine Quality"
subtitle: "Final Project"
author: "YoungRi Lee & Jihyun Lee"
date: "Last compiled on `r Sys.Date()`"
output:
  bookdown::pdf_document2:
    fig_caption: yes
    includes:
      in_header: addons/style.sty
    toc: no
    citation_package: biblatex
  pdf_document:
    toc: no
bibliography: addons/reference.bib
editor_options: 
  markdown: 
    wrap: sentence
---

```{r message = F, warning = F, echo = F}
library(dplyr)
library(kableExtra)
library(ggplot2)
library(glmnet)
library(latex2exp)
library(rpart)
library(rpart.plot)
library(plotrix)
rm(list = ls())
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Introduction

## Data

```{r, echo = F}
# Read data
red <- read.csv("Wine quality/winequality-red.csv", sep = ";")
white <- read.csv("Wine quality/winequality-white.csv", sep = ";")

# create wine type indicator (red = 1, white = 0: dummy variable)
red <- red %>% mutate(red = 1)
white <- white %>% mutate(red = 0)

# create a full dataset
data <- rbind(red, white)

# change to factor variable (dummy)
data <- data %>% mutate(red = factor(as.character(red), labels = c("White", "Red"))) 
#table(data$red)

```

This wine quality dataset is from [@Cortez2009].
The dataset includes *vinho verde*, a unique product from the Minho (northwest) region of Portugal.
The data were collected from May/2004 to February/2007 using only protected designation of origin samples that were tested at the official certification entity (CVRVV).
The CVRVV is an inter-professional organization with the goal of improving the quality and marketing of vinho verde.
The data were recorded by a computerized system (iLab), which automatically manages the process of wine sample testing from producer requests to laboratory and sensory analysis.

The outcome variable is `wine quality`, which was measured by a minimum score of three sensory assessors using blind tastes in a scale that ranges from 0 (very bad) to 10 (excellent).
There are 11 attributes of the wine based on physicochemical tests: fixed acidity (g(tartaric acid)/$dm^3$), volatile acidity (g(acetic acid)/$dm^3$), citric acid (g/$dm^3$), residual sugar (g/$dm^3$), chlorides (g(sodium chloride)/$dm^3$), free sulfur dioxide (mg/$dm^3$), total sulfur dioxide (mg/$dm^3$), density (g/$dm^3$), pH, sulphates (g(potassium sulphate)/$dm^3$), and alcohol (vol.%).
Originally, two datasets were created separately, one for red wine ($n = 1599$) and another for white wine ($n = 4988$).
In this report, we use a merged dataset and create a dummy variable to indicate the wine type, `red`.
Thus, in total, the dataset includes 11 numerical attributes (covariates), one dummy variable, and one numerical outcome.
There is no missing value in this dataset.

Table \@ref(tab:desc) shows the descriptive statistics of 11 attributes by wine type.
[ADD SOME DESCRIPTION]

```{r, echo=FALSE}
data_cov <- data[,c(1:11,13)]

options(dplyr.summarise.inform = FALSE)

## Min
Min <- data_cov %>% group_by(red) %>%
  summarise(across(fixed.acidity:alcohol, ~ min(.x))) %>%
   mutate_if(is.numeric, round, 3)
Min <- as.data.frame(t(as.matrix(Min)))
colnames(Min) <- c("Min", "Min")

## Max
Max <- data_cov %>% group_by(red) %>%
  summarise(across(everything(), ~  max(.x)))%>%
   mutate_if(is.numeric, round, 3)
Max <- as.data.frame(t(as.matrix(Max)))
colnames(Max) <- c("Max", "Max")

## Mean
Mean <- data_cov %>% group_by(red) %>%
  summarise(across(everything(), ~  mean(.x)))%>%
   mutate_if(is.numeric, round, 3)
Mean <- as.data.frame(t(as.matrix(Mean)))
colnames(Mean) <- c("Mean", "Mean")

Descriptive <- cbind(Min, Max, Mean)
White <- Descriptive[,c(1,3,5)]
Red <- Descriptive[,c(2,4,6)] 

Descriptive_f <- cbind(White, Red)[2:12,]
rownames(Descriptive_f) <- c("Fixed acidity", "Volatile acidity", "Citric acid", "Residual sugar", "Chlorides", "Free sulfur dioxide", "Total sulfur dioxide", "Density", "pH", "Sulphates", "Alcohol")

# Final table
kbl(Descriptive_f, caption = "\\label{tab:desc} Descriptive statistics of 11 attributes", booktabs = T, escape = FALSE, format = "latex") %>% kable_styling(full_width = F) %>% 
  add_header_above(c("", "White" = 3, "Red" = 3)) %>%
  footnote(general = "The scale of each attribute is given in the text.")
```

Figure 1 shows the distribution of wine quality by wine type.
Generally, it shows a normal shape distribution and centered around the middle point of the scale.
Red wine has fewer observations than white wine.

```{r echo = F, fig.width=8, fig.height=3}
## Need to reduce the size or add it as latex format.
## Here's color chart: http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/
data %>% ggplot() + 
  geom_histogram(aes(x = quality, fill = red), binwidth = 1, alpha = .7, position = "identity") +
  theme_minimal() +
  labs(title = "Figure 1. Histrogram of wine quality by wine type", 
       x = "Quality", fill = "Wine") +
  scale_x_continuous(breaks=seq(3, 9, 1)) +
  scale_fill_manual(values=c("#669966", "#993366"))
```

## Research Questions

Using this dataset, we would like to (1) build the best model to predict wine quality, (2) examine the most influential set of attributes to predict wine quality.

To achieve this goal, we implemented two classes of techniques: shrinkage approaches (i.e., Ridge and Lasso regressions) and tree-based approaches (i.e., regression tree).
This report includes the comparisons of two approaches, model selection procedures (e.g., cross-validation), and proposal of the best model.


<!-- 1. What model best predicts the wine quality? -->

<!-- 2. Which attributes are the most influential to predict wine quality? -->

# Methods

We implemented shrinkage approach (i.e., Ridge and Lasso regressions) and tree-based approach (i.e., Regression tree) to build a prediction model.

## Shrinkage approaches

The shrinkage method uses all possible covariates while shrinking the covariates' coefficients towards zero that does not associate with the outcome as strong as other predictors.
The advantage of this method is to reduce the variance by reducing the relatively unimportant attributes and selecting variables.
Thus, important covariates in the model can be emphasized.
However, it may lead to the bias of the model by reducing the variance (\textit{bias-variance trade-off}).
Generally preferred shrinkage methods are \textit{ridge regression} and \textit{lasso}.

### Ridge regression

The purpose of ridge regression is to reduce variance of the predictions by minimizing the residual sum of squares (RSS) of the model as well as the shrinkage penalty:
$$
RSS + \lambda\sum^p_{j=1}{\beta^2_j},
$$
where $RSS = \sum^n_{i=1}(y_i-\beta_0-\sum^p_{j=1}\beta_jx_{ij})^2$ and the second term indicates the shrinkage penalty with a \textit{tuning parameter} $\lambda$.
As the coefficients of the attributes become close to zero, the model fit of ridge regression will be desirable.
The value of $\lambda$ controls the impact of shrinkage penalty.
As the value of $\lambda$ increases, the impact of the shrinkage penalty grows and the ridge regression coefficients will be close to zero.
<!-- In other words, the ridge regression uses all covariates in the data since the coefficient becomes not equal to zero. -->
Generally, the value of $\lambda$ is selected using cross-validation to find the optimal value.
Ridge regression has a drawback in using all variables in the data and cannot select or subset the important variable, because the coefficients \textit{shrinks toward} zero, not become exact zero. This can lead a challenge in interpretation of the model with large set of variables. 


### Lasso

Lasso is similar to Ridge regression as it shrinks the coefficients toward zero but has a more stringent shrinkage penalty that can force the coefficients to be equal to zero:

$$
RSS + \lambda\sum^p_{j=1}{|\beta^2_j|}.
$$

By pushing the coefficient to be zero, lasso performs variable selection identifying the variables that substantially impact the outcome.
<!-- When there are a small number of variables with strong effect, lasso regression has a benefit over Ridge regression by selecting only those variables. -->
Thus, the model using lasso might be more parsimonious than that using ridge regression, making the model results more interpretable.
Like ridge regression, the value of $\lambda$ in lasso is critical and will be determined by the cross-validation.


## Tree-based approaches

The tree-based approaches can be used for regression and classification.
We will focus on regression trees because the outcome variable (\textit{wine quality}) is a numeric in our data.

### Regression trees

Tree-based approaches segment the predictor space into several simple regions.
In a tree, each rule that split the segment will be summarized, called decision-tree methods.
After splitting the predictor space into a number of regions (a leaf node; $R_1, R_2, ... R_J$) using the selected predictors $X_j$, the average of the outcome within each region ($\hat{y}_{R_j}$) will be calculated.
The model fit of the regression tree aims to minimize the RSS:

$$
\sum^J_{j=1}{\sum_{i \in R_j}{(y_i-\hat{y}_{R_j})^2}}
$$

Thus, the regression trees split the predictor space and create the two new branches only if the new split decreases RSS.
Since the regression trees use this top-down, greedy approach to select the model, the final model might not be the true optimal model.
However, the regression trees have the advantage of taking into account the nonlinearity or interaction of covariates.
Also, the regression tree is useful in interpreting the results.


# Results

## Ridge regression

```{r, fig.width=6, fig.height=4, echo=FALSE}
# data
X <- model.matrix(quality ~ . -1, data = data)
y <- data$quality
p <- ncol(X) ##### Why p = 13? 

# fit ridge
fit_ridge <- glmnet(X, y, alpha = 0) # Standardize variables by default

# plot
plot(fit_ridge, xvar = "lambda", label = TRUE, xlab = bquote('log('*lambda*')'), ylab = bquote(beta[j]), main = 'Ridge coefficients')

```


$X_8$ is `density`. (Humm...)

```{r, echo = F}
hist(data$density)
```



```{r, echo = F}
######## Cross-validation to choose tuning parameter

# get lambda_grid for later
lambda_grid <- fit_ridge$lambda 

# cv
cv_ridge <- cv.glmnet(X, y, alpha = 0)
plot(cv_ridge, xlab = TeX("$\\log (\\lambda)$"), ylab = 'CV Error', main = "Ridge CV error")


ridge_lambda_min <- cv_ridge$lambda.min
```

The smallest cross-validation error minimizes with $\lambda =$ `r round(ridge_lambda_min,4)`.

```{r}
# hand-written cv
# cv function
cv_penalized_reg <- function(X, y, lambda_grid, K = floor(sqrt(nrow(X)))){
 
  n <- nrow(X)
  p <- ncol(X)
  folds <- cut(sample(1:n), breaks = K, labels = FALSE)

  train_err <- cv_err <- array(NA, dim = c(K, length(lambda_grid)))
  
  for(k in 1:K){
    idx_tr <- which(folds != k)
    idx_ts <- which(folds == k)
    
    fit_glm <- glmnet(X[idx_tr,], y[idx_tr], alpha = 0, lambda = lambda_grid)
    
    if (length(idx_ts) > 1){
      cv_err[k,] <- as.numeric(colMeans((predict(fit_glm, X[idx_ts,], 
                               lambda = lambda_grid) - y[idx_ts])^2))
    }
    else{
      cv_err[k,] <- as.numeric(colMeans((predict(fit_glm, 
                    matrix(X[idx_ts,],1,p), 
                    lambda = lambda_grid) - y[idx_ts])^2))
    }
  }
  cv_MSE <- colMeans(cv_err)
  sd_cv_MSE <- apply(cv_err, 2, sd)/sqrt(K)
  
  return(list('cv_MSE' = cv_MSE, 'sd_cv_MSE' = sd_cv_MSE, 'cv_err' = cv_err))
}

# find the best model 
fit_err <- cv_penalized_reg(X, y, lambda_grid, K = 10)

# plot and see which one is the optimal & conservative one.
plotCI(x = log(lambda_grid), y = fit_err$cv_MSE, uiw = fit_err$sd_cv_MSE, 
       col = 'red', scol = 'gray', pch = 16, lwd = 2, 
       xlab = TeX("$\\log (\\lambda)$"), ylab = 'Error')
abline(v = log(lambda_grid)[which.min(fit_err$cv_MSE)], lwd = 1, col = 1, lty = 2)
abline(h = min(fit_err$cv_MSE) + fit_err$sd_cv_MSE[which.min(fit_err$cv_MSE)], 
       lwd = 1, lty = 2)
idx <- which( min(fit_err$cv_MSE) + fit_err$sd_cv_MSE[which.min(fit_err$cv_MSE)] >= fit_err$cv_MSE)
idx <- idx[1]
abline(v = log(lambda_grid[idx]), lwd = 1, col = 1, lty = 3)
legend('topleft', legend = 'Ridge CV Error', pch = 16, col = 'red')

cv10_ridge <- fit_err$cv_err[,which.min(fit_err$cv_MSE)] # optimal
compare <- data.frame(cv10_ridge) %>% mutate(model = "Ridge regression") %>% 
  rename(value = cv10_ridge) %>% 
  select(model, value)
```


## Lasso regression

```{r, fig.width=6, fig.height=4, echo=FALSE}
# fit lasso
fit_lasso <- glmnet(X, y)
plot(fit_lasso, xvar = "lambda", label = TRUE, 
     xlab = bquote('log('*lambda*')'), ylab = bquote(beta[j]), 
     main = 'Lasso coefficients')

```

```{r}
######## CV to choose tuning parameter ###########




lambda_grid <- fit_lasso$lambda # get lambda_grid for later

# cv
cv_lasso <- cv.glmnet(X, y)
plot(cv_lasso, xlab = TeX("$\\log (\\lambda)$"), ylab = 'CV Error')

# hand-written cv
# cv function
cv_penalized_reg <- function(X, y, lambda_grid, K = floor(sqrt(nrow(X)))){
 
  n <- nrow(X)
  p <- ncol(X)
  folds <- cut(sample(1:n), breaks = K, labels = FALSE)

  train_err <- cv_err <- array(NA, dim = c(K, length(lambda_grid)))
  
  for(k in 1:K){
    idx_tr <- which(folds != k)
    idx_ts <- which(folds == k)
    
    fit_glm <- glmnet(X[idx_tr,], y[idx_tr], lambda = lambda_grid)
    
    if (length(idx_ts) > 1){
      cv_err[k,] <- as.numeric(colMeans((predict(fit_glm, X[idx_ts,], 
                               lambda = lambda_grid) - y[idx_ts])^2))
    }
    else{
      cv_err[k,] <- as.numeric(colMeans((predict(fit_glm, 
                    matrix(X[idx_ts,],1,p), 
                    lambda = lambda_grid) - y[idx_ts])^2))
    }
  }
  cv_MSE <- colMeans(cv_err)
  sd_cv_MSE <- apply(cv_err, 2, sd)/sqrt(K)
  
  return(list('cv_MSE' = cv_MSE, 'sd_cv_MSE' = sd_cv_MSE, 'cv_err' = cv_err))
}

# find the best model 
fit_err <- cv_penalized_reg(X, y, lambda_grid, K = 10)

# plot and see which one is the optimal & conservative one.
plotCI(x = log(lambda_grid), y = fit_err$cv_MSE, uiw = fit_err$sd_cv_MSE, 
       col = 'red', scol = 'gray', pch = 16, lwd = 2, 
       xlab = TeX("$\\log (\\lambda)$"), ylab = 'Error')
abline(v = log(lambda_grid)[which.min(fit_err$cv_MSE)], lwd = 1, col = 1, lty = 2)
abline(h = min(fit_err$cv_MSE) + fit_err$sd_cv_MSE[which.min(fit_err$cv_MSE)], 
       lwd = 1, lty = 2)
idx <- which( min(fit_err$cv_MSE) + fit_err$sd_cv_MSE[which.min(fit_err$cv_MSE)] >= fit_err$cv_MSE)
idx <- idx[1]
abline(v = log(lambda_grid[idx]), lwd = 1, col = 1, lty = 3)
legend('topleft', legend = 'Lasso CV Error', pch = 16, col = 'red')

cv10_lasso <- fit_err$cv_err[,which.min(fit_err$cv_MSE)] # optimal
compare <- data.frame(cv10_lasso) %>% mutate(model = "Lasso regression") %>% 
  rename(value = cv10_lasso) %>% 
  select(model, value) %>% bind_rows(compare)

# comparison 
compare$model <- factor(compare$model , levels=c("Ridge regression", "Lasso regression"))
ggplot(compare, aes(x = model, y = value, fill = model)) +
  geom_boxplot(width = 0.5, alpha = 0.7) + 
  labs(title="Test error by Ridge and Lasso regressions",
       x ="Model", y = "Test Error") + 
  scale_fill_manual(values=c("#E41A1C", "#377EB8")) + # "#4DAF4A", "#984EA3"
  theme_minimal() +
  theme(legend.title = element_blank())
```


## Tree-based approaches
```{r, fig.width=6, fig.height=4, echo=FALSE}
# Split the data in training and test
n <- nrow(data)
n_train <- floor(0.8 * n)
n_test <- n - n_train

set.seed(123)
idx_test <- sample(1:n, n_test, replace = F)
data_ts <- data[idx_test,]
data <- data[-idx_test,]

# fit tree
best_tree <- rpart(quality ~ ., method = "anova", data = data)
cat('Size of optimal tree:', length(unique(best_tree$where)), '\n')

# Plot the cv error curve for the tree
par(mar=c(4,4,4,2), family = 'serif')
plotcp(best_tree)

# Show the optimal tree
rpart.plot(best_tree, clip.right.labs = FALSE, under = TRUE, digits = 4)

# Prediction on the test set
pred_y <- as.numeric(predict(best_tree, newdata = data_ts))
err_all <- (pred_y - data_ts$quality)^2

plot(data_ts$quality, pred_y, pch = 16, xlab = 'observed', ylab = 'predicted')
abline(0, 1, lwd = 2, col = 'red') 
```

# Discussion

## Limitations

-   Only uses the wine data from *vinho verde* region of Portugal. If a test dataset outside of this region is available, we would be able to test to generalize the results to a broader range of wine.

## Implications

-   Wine producers will use this information to produce the better quality of wine considering the selected attributes.

-   Wine consumers will be able to choose a good quality of wine without tasting (e.g., via online shopping) if the physicochemical information of wine is available.
