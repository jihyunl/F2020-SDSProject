---
title: "Modeling to Predict Wine Quality"
subtitle: "SDS 323 Final Project"
author: "YoungRi Lee (yl29982) & Jihyun Lee (jl64875)"
output:
  bookdown::pdf_document2:
    fig_caption: yes
    includes:
      in_header: addons/style.sty
    toc: no
    citation_package: biblatex
    extra_dependencies: "subfig"
  pdf_document:
    toc: no
bibliography: addons/reference.bib
editor_options: 
  markdown: 
    wrap: sentence
---

```{r message = F, warning = F, echo = F}
library(dplyr)
library(kableExtra)
library(ggplot2)
library(glmnet)
library(latex2exp)
library(rpart)
library(rpart.plot)
library(plotrix)
library(randomForest)
library(gbm)
library(xgboost)

rm(list = ls())
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Introduction

```{r, echo = F}
# read data
red <- read.csv("Wine quality/winequality-red.csv", sep = ";")
white <- read.csv("Wine quality/winequality-white.csv", sep = ";")

# create wine type indicator (red = 1, white = 0: dummy variable)
red <- red %>% mutate(red = 1)
white <- white %>% mutate(red = 0)

# create a full dataset
data <- rbind(red, white)

# change to factor variable (dummy)
data <- data %>% mutate(red = factor(as.character(red), labels = c("White", "Red"))) 
```

We used a wine quality dataset from Cortez et al. (2009) [@Cortez2009]. The dataset only includes *vinho verde*, a unique product from the Minho (northwest) region of Portugal. 
The outcome variable is `wine quality`, measured by a minimum score of three sensory assessors using blind tastes in a scale ranging from 0 (very bad) to 10 (excellent). 
There are 11 attributes of the wine based on physicochemical tests (Table 1). Initially, two datasets of red wine ($n = 1,599$) and white wine ($n = 4,988$) were available. We merged the two and used for the analyses by creating an additional dummy variable to indicate the wine type, `red`. There is no missing value in this dataset.
Figure 1 shows the distribution of wine quality by wine type. Generally, it shows a normal shape distribution and centered around the middle point of the scale. Red wine has fewer observations than white wine.

```{r, echo=FALSE}
options(dplyr.summarise.inform = FALSE)

## Min
Min <- data %>% group_by(red) %>%
  summarise(across(fixed.acidity:quality, ~ min(.x))) %>%
   mutate_if(is.numeric, round, 3)
Min <- as.data.frame(t(as.matrix(Min)))
colnames(Min) <- c("Min", "Min")

## Max
Max <- data %>% group_by(red) %>%
  summarise(across(everything(), ~  max(.x)))%>%
   mutate_if(is.numeric, round, 3)
Max <- as.data.frame(t(as.matrix(Max)))
colnames(Max) <- c("Max", "Max")

## Mean
Mean <- data %>% group_by(red) %>%
  summarise(across(everything(), ~  mean(.x)))%>%
   mutate_if(is.numeric, round, 3)
Mean <- as.data.frame(t(as.matrix(Mean)))
colnames(Mean) <- c("Mean", "Mean")

Descriptive <- cbind(Min, Max, Mean)
White <- Descriptive[,c(1,3,5)]
Red <- Descriptive[,c(2,4,6)] 

Descriptive_f <- cbind(White, Red)[2:13,]
rownames(Descriptive_f) <- c("Fixed acidity", "Volatile acidity", "Citric acid", "Residual sugar", "Chlorides", "Free sulfur dioxide", "Total sulfur dioxide", "Density", "pH", "Sulphates", "Alcohol", "Wine Quality")

# Final table
kbl(Descriptive_f, caption = "\\label{tab:desc} Descriptive statistics of 11 attributes and outcome",
    booktabs = T, escape = FALSE, format = "latex", align = "lcccccc") %>% 
  kable_styling(full_width = F, font_size = 7) %>% 
  add_header_above(c("", "White" = 3, "Red" = 3))
```


```{r, include = F, out.width= "60%",  fig.cap='Histrogram of wine quality by wine type', fig.align="center"}
# Here's color chart: http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/
data %>% ggplot() + 
  geom_histogram(aes(x = quality, fill = red), binwidth = 1, alpha = .7, position = "identity") +
  theme_minimal() +
  labs(x = "Quality", fill = "Wine") +
  scale_x_continuous(breaks=seq(3, 9, 1)) +
  scale_fill_manual(values=c("#669966", "#993366"))
```


```{r echo = F, out.width= "50%",  fig.cap='Histrogram of wine quality by wine type', fig.align="center"}
## Here's color chart: http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/
data %>% ggplot() + 
  geom_histogram(aes(x = quality, fill = red), binwidth = 1, alpha = .7, position = "identity") +
  theme_minimal() +
  labs(x = "Quality", fill = "Wine") +
  scale_x_continuous(breaks=seq(3, 9, 1)) +
  scale_fill_manual(values=c("#669966", "#993366"))
```

Our research goals are as follows: (1) to build the best model to predict wine quality, and (2) to examine the most influential set of attributes to predict wine quality. We implemented three techniques: shrinkage approach (i.e., Lasso) and tree-based approaches (i.e., Random Forest and Boosting). Based on the comparison of the alternative methods, we proposed the best model for predicting wine quality and suggest the most influential variables on the prediction.

# Methods

## Shrinkage Method: Lasso

The two well-known shrinkage methods are \textit{Ridge regression} and the \textit{Lasso}. 
The benefit of these methods is to reduce the variance by shrinking the relatively unimportant attributes and selecting variables. However, it may lead to the bias of the model by reducing the variance (\textit{bias-variance trade-off}). We choose Lasso over Ridge regression because Lasso forces the coefficients to be equal to zero, and it outperforms ridge when variables are highly correlated.
In Lasso, $\lambda$ is a \textit{tuning parameter}, which controls the impact of shrinkage penalty. We selected $\lambda$ by the cross-validation.


## Tree-based Methods

Tree-based methods can be used for regression and classification. This method segments the predictor space into several simple regions, and each rule that split the segment is summarized. We  used two of ensemble methods, **random forests** and **boosting**, instead of growing a single tree. The ensemble methods are more powerful and useful than a single tree to build prediction models. 

The random forests, is an improved version of \textit{bagged} trees, which build a number of decision trees using bootstrapped training samples. Random forests are similar to bagging but use only a random subset of predictors. This process decorrelates the built trees in bagging and reduces the variance. In this study, we will use $m=\sqrt{p}=\sqrt{12}$ as the number of predictors considered at each split.

In boosting, the trees are grown sequentially, and each tree is fitted on the residuals from the previous tree. The final model in boosting is determined by the sum of all fitted trees. While fitting boosting, we specified three parameters: shrinkage parameter ($\lambda$), number of iterations ($B$), and the size of each new tree ($d$, the number of splits).

# Results

## Shrinkage Method: Lasso

```{r, fig.width=5, fig.height=3, echo=FALSE}
# data
X <- model.matrix(quality ~ . -1, data = data)
y <- data$quality
p <- ncol(X) # = 13? 

# fit lasso
fit_lasso <- glmnet(X, y, alpha = 1)
```


```{r}
######## CV to choose lambda ###########
# CV
set.seed(201204)
cv_lasso <- cv.glmnet(X, y, alpha = 1)
mse_lasso <- cv_lasso$cvm[which.min(cv_lasso$cvm)]
```

```{r, echo = F, fig.cap = 'Lasso results', out.width= "50%", fig.subcap=c('Lasso coefficients', 'CV error'), fig.pos="h"}
# plot lasso
plot(fit_lasso, xvar = "lambda", label = TRUE, 
     xlab = bquote('log('*lambda*')'), ylab = bquote(beta[j]))

## plot
plot(cv_lasso, xlab = TeX("$\\log (\\lambda)$"), ylab = 'CV Error')
```


```{r}
##### Result interpretation
# minimum lambda
minlam <- cv_lasso$lambda.min
```

Figure 2(a) shows the lasso coefficients across the $log(\lambda)$. Only one variable, `density`, shows rapid change as $log(\lambda)$ increases and other covariates are close to zero. We use 10-fold cross-validation (CV) to choose the tuning hyperparameter, $\lambda$ that minimizes the expected out-of-sample prediction error. 
Figure 2(b) shows CV error as $log(\lambda)$ changes. We chose the optimal model with the minimum CV error, $\hat{\lambda} =$ `r minlam` (Mean squared error; $MSE = .539$). The optimal model selects all twelve predictors in the model. 


```{r, include=FALSE}
# prediction
lasso_coef <- predict(fit_lasso, s = minlam, type = "coefficients")[1:13,]

lasso_coef <- as.data.frame(lasso_coef)
Variables <- rownames(lasso_coef)
lasso_coef <- lasso_coef %>% mutate(Variables = Variables)
lasso_coef <- lasso_coef %>% mutate_if(is.numeric, round, 3) %>% select(Variables, lasso_coef)
  
kbl(lasso_coef, caption = "Lasso coefficients for the optimal lambda", 
    booktabs = T, format = "latex", 
    col.names = c("Variables", "Coefficients")) %>% 
  kable_styling(full_width = F, font_size = 8)
```


## Tree-based Method: Random forests

```{r, fig.width=6, fig.height=4, echo=FALSE}
# Split the data in training and test
n <- nrow(data)
n_train <- floor(0.8 * n)
n_test <- n - n_train

set.seed(123)
idx_test <- sample(1:n, n_test, replace = F)
data_tests <- data[idx_test,]
data_train <- data[-idx_test,]
```


```{r echo = F, fig.cap = 'Random forest', out.width= "50%", fig.pos="h", fig.subcap=c("Out-of-bag Error", "Prediction"), fig.align="center"}
set.seed(1205)
forest<- randomForest(quality~., data = data_train, importance=TRUE, mtry = sqrt(12))

plot(forest)

## Test set
pred_y_forest <- predict(forest, newdata = data_tests)

plot(data_tests$quality, pred_y_forest, pch = 16, xlab = 'observed', ylab = 'predicted', 
     xlim = c(3.5, 8), ylim = c(3.5, 8))
abline(0, 1, lwd = 2, col = 'red')

err_forest <- (pred_y_forest - data_tests$quality)^2

# MSE
mse_forest <- mean(err_forest)
```

Figure 3(a) illustrates test (out-of-bag; OOB) error estimation, which suggests OOB error stabilizes before 200 trees. Based on the prediction of random forest, we can see a decent fit of the model in Figure 3(b) with MSE of `r round(mse_forest,3)`. 

```{r fig.cap="Random Forests: Variable Importance Plot", out.width="60%", fig.align="center"}
varImpPlot(forest)
```

In the left panel of Figure 4, `%IncMSE` indicates the mean decrease of accuracy in prediction on the out of bag samples when a given variable is excluded from the model. The more considerable reduction indicates that the target variable is important in the model. `IncNodePurity` measures the total decrease in training RSS (the right panel of Figure 4). Based on the random forests, `Alcohol` seems to be the most important variable and wine type (`red`) seems to be the least important variable in the model.

## Tree-based Method: Boosting

```{r, eval=FALSE}
n <- nrow(data)

set.seed(1205)
boost <- gbm(quality~., data = data_train, distribution = "gaussian", n.trees = 2000, interaction.depth = 4)
# summary(boost)

# par(mfrow=c(1,2)) 
# plot(boost, i = "alcohol") 

# predict
yhat.boost=predict(boost,newdata=data_tests, n.trees=2000)

# MSE
mse_boost <- mean((yhat.boost -data_tests$quality)^2)
```

```{r, eval = F}
data_X <- data %>% mutate(red = as.numeric(red))
  
data_X$quality <- NULL

data_xgb <- xgb.DMatrix(as.matrix(data_X), label = data$quality)

# create hyperparameter grid
hyper_grid <- expand.grid(
  eta = c(.01, .05, .15, 0.3), # fit different eta
  max_depth = c(1, 3, 5, 7),
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)


set.seed(1205)
# grid search
for(i in 1:nrow(hyper_grid)) {

  # create parameter list
  params <- list(
    eta = hyper_grid$eta[i],
    max_depth = hyper_grid$max_depth[i]
  )

  # reproducibility
  set.seed(123)

  # train model
  xgb.tune <- xgb.cv(
    params = params,
    data = data_xgb,
    nrounds = 5000,
    nfold = 5,
    objective = "reg:squarederror",  # for regression models
    verbose = 0,               # silent,
    early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
  )

  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(xgb.tune$evaluation_log$test_rmse_mean)
  hyper_grid$min_RMSE[i] <- min(xgb.tune$evaluation_log$test_rmse_mean)
}

save(hyper_grid, file = "boosting.rds")
```

```{r}
load("boosting.rds")

kbl(hyper_grid[order(hyper_grid$min_RMSE, decreasing = F),][1:5,], 
    booktabs = T, format = "latex", 
    caption = "Boosting results", digits = 3) %>% 
  kable_styling(full_width = F, font_size = 7)
```



```{r}
data_X <- data %>% mutate(red = as.numeric(red))
  
data_X$quality <- NULL

data_xgb <- xgb.DMatrix(as.matrix(data_X), label = data$quality)

# parameter list
params <- list(
  eta = hyper_grid$eta[which.min(hyper_grid$min_RMSE)],
  max_depth = hyper_grid$max_depth[which.min(hyper_grid$min_RMSE)]
)

# train final model
xgb.fit.final <- xgboost(
  params = params,
  data = data_xgb,
  nrounds = hyper_grid$optimal_trees[which.min(hyper_grid$min_RMSE)],
  objective = "reg:squarederror",
  verbose = 0
)
```

```{r fig.cap="Boosting", out.width= "50%", fig.pos="h", fig.subcap=c("Variable Importance Plot", "Prediction")}
# create importance matrix
importance_matrix <- xgb.importance(model = xgb.fit.final)

# variable importance plot
xgb.plot.importance(importance_matrix, top_n = 10, measure = "Gain")


## Prediction
data_ts_X <- data_tests  %>% mutate(red = as.numeric(red))
data_ts_X$quality <- NULL

data_test_xgb <- xgb.DMatrix(as.matrix(data_ts_X), label = data_tests$quality)
pred_y_boost <- predict(xgb.fit.final, data_test_xgb)

# plot
plot(data_tests$quality, pred_y_boost, pch = 16, xlab = 'observed', ylab = 'predicted')
abline(0, 1, lwd = 2, col = 'red')

# MSE
err_boost <- (pred_y_boost - data_tests$quality)^2
mse_boost <-mean(err_boost) # MSE
```

We chose an optimal lambda using the CV approach. Table 2 displays the models with the first five minimum RMSE. The final model is determined based on minimum RMSE (0.602) with $\eta = .05$. 
Figure 5(a) shows `alcohol` and `volatile.acidity` are important variables, which are similar to that of random forests. The MSE of boosting model is `r round(mse_boost, 3)` [Figure 5(b)].

# Conclusion

```{r}
data.frame(Lasso = mse_lasso,
           RF = mse_forest,
           Boosting = mse_boost) %>% 
  kbl(col.names = c("Lasso", "Random Forest", "Boosting"),
      booktabs = T, 
      caption = "MSE Comparison", digits = 3) %>% 
  kable_styling(full_width = FALSE, font_size = 8)
```

In this project, we explored the best model to predict wine quality using a real data. 
We compared lasso, random forests, and boosting, examining each model with the best fit using cross-validation. 
Overall, tree-based methods showed a better performance than lasso regression (Table 3). Among the tree-based methods, boosting showed a smaller MSE compared to random forests. We concluded that `alcohol` and `volatile.acidity` as the critical variables that influence the wine quality based on the random forests and boosting results. 
Wine producers will be able to use this information to produce a better wine quality, considering the selected attributes. Also, wine consumers will be able to choose a good quality wine without tasting (e.g., via online shopping) if they are provided the physicochemical information about the wine.


